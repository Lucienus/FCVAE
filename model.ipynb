{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06328c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load model.py\n",
    "from pytorch_lightning import LightningModule\n",
    "from torch.utils.data import DataLoader\n",
    "from CVAE import CVAE\n",
    "from dataset import UniDataset\n",
    "import argparse\n",
    "from torch import optim\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import data_augment\n",
    "from get_f1_score import best_f1, delay_f1, best_f1_without_pointadjust\n",
    "from Attention import EncoderLayer_selfattn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "class MyVAE(LightningModule):\n",
    "    \"\"\"Frequency-enhenced CVAE\"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super(MyVAE, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.hp = hparams\n",
    "        self.__build_model()\n",
    "\n",
    "    def __build_model(self):\n",
    "        self.vae = CVAE(self.hp)\n",
    "        self.atten = nn.ModuleList(\n",
    "            [\n",
    "                EncoderLayer_selfattn(\n",
    "                    self.hp.d_model,\n",
    "                    self.hp.d_inner,\n",
    "                    self.hp.n_head,\n",
    "                    self.hp.d_model // self.hp.n_head,\n",
    "                    self.hp.d_model // self.hp.n_head,\n",
    "                    dropout=0.1,\n",
    "                )\n",
    "                for _ in range(1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mode, mask):\n",
    "        x = x.view(-1, 1, self.hp.window)\n",
    "        return self.vae.forward(x, mode, mask)\n",
    "\n",
    "    def loss(self, x, y_all, z_all, mode=\"train\"):\n",
    "        y = (y_all[:, -1]).unsqueeze(1)\n",
    "        mask = torch.logical_not(torch.logical_or(y_all, z_all))\n",
    "        mu_x, var_x, rec_x, mu, var, loss = self.forward(\n",
    "            x,\n",
    "            \"train\",\n",
    "            mask,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, data_batch, batch_idx):\n",
    "        x, y_all, z_all = data_batch\n",
    "        x, y_all, z_all = self.batch_data_augmentation(x, y_all, z_all)\n",
    "        loss_val = self.loss(x, y_all, z_all)\n",
    "        if self.trainer.strategy == \"dp\":\n",
    "            loss_val = loss_val.unsqueeze(0)\n",
    "        self.log(\"val_loss_train\", loss_val, on_step=True, on_epoch=False, logger=True)\n",
    "        output = OrderedDict(\n",
    "            {\n",
    "                \"loss\": loss_val,\n",
    "            }\n",
    "        )\n",
    "        return output\n",
    "\n",
    "    def validation_step(self, data_batch, batch_idx):\n",
    "        x, y_all, z_all = data_batch\n",
    "        loss_val = self.loss(x, y_all, z_all)\n",
    "        if self.trainer.strategy == \"dp\":\n",
    "            loss_val = loss_val.unsqueeze(0)\n",
    "        self.log(\"val_loss_valid\", loss_val, on_step=True, on_epoch=True, logger=True)\n",
    "        output = OrderedDict(\n",
    "            {\n",
    "                \"loss\": loss_val,\n",
    "            }\n",
    "        )\n",
    "        return output\n",
    "\n",
    "    def test_step(self, data_batch, batch_idx):\n",
    "        x, y_all, z_all = data_batch\n",
    "        y = (y_all[:, -1]).unsqueeze(1)\n",
    "        with torch.no_grad():\n",
    "            mu_x_test, recon_prob = self.forward(x, \"test\", z_all)\n",
    "            mask = torch.logical_not(z_all)\n",
    "            mu_x, var_x, rec_x, mu, var, loss = self.forward(x, \"train\", mask)\n",
    "        recon_prob = recon_prob[:, :, -1]\n",
    "        output = OrderedDict(\n",
    "            {\n",
    "                \"y\": y.cpu(),\n",
    "                \"recon_prob\": recon_prob.cpu(),\n",
    "                \"mu_x\": mu_x[:, :, -1].cpu(),\n",
    "                \"mu_x_test\": mu_x_test[:, :, -1].cpu(),\n",
    "                \"x\": x[:, :, -1].cpu(),\n",
    "                \"var_x\": var_x[:, :, -1].cpu(),\n",
    "            }\n",
    "        )\n",
    "        return output\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        y = torch.cat(([x[\"y\"] for x in outputs]), 0)\n",
    "        recon_prob = torch.cat(([x[\"recon_prob\"] for x in outputs]), 0)\n",
    "        x = torch.cat(([x[\"x\"] for x in outputs]), 0)\n",
    "        mu_x = torch.cat(([x[\"mu_x\"] for x in outputs]), 0)\n",
    "        mu_x_test = torch.cat(([x[\"mu_x_test\"] for x in outputs]), 0)\n",
    "        var_x = torch.cat(([x[\"var_x\"] for x in outputs]), 0)\n",
    "        score = -1 * recon_prob.squeeze(1).cpu().numpy()\n",
    "        label = y.squeeze(1).cpu().numpy()\n",
    "        df = pd.DataFrame()\n",
    "        df[\"x\"] = x.cpu().numpy().reshape(-1)\n",
    "        df[\"mu_x\"] = mu_x.cpu().numpy().reshape(-1)\n",
    "        df[\"mu_x_test\"] = mu_x_test.cpu().numpy().reshape(-1)\n",
    "        df[\"var_x\"] = var_x.cpu().numpy().reshape(-1)\n",
    "        df[\"y\"] = y.cpu().numpy().reshape(-1)\n",
    "        df[\"recon\"] = score.reshape(-1)\n",
    "        np.save(\"./npy/score.npy\", score)\n",
    "        np.save(\"./npy/label.npy\", label)\n",
    "        if self.hp.data_dir == \"./data/Yahoo\":\n",
    "            k = 3\n",
    "        elif self.hp.data_dir == \"./data/NAB\" or self.hp.data_dir == \"./data/new_NAB\":\n",
    "            k = 150\n",
    "        else:\n",
    "            k = 7\n",
    "        auc = roc_auc_score(label, score)\n",
    "        delay_f1_score, delay_precison, delay_recall, delay_predict = delay_f1(\n",
    "            score, label, k\n",
    "        )\n",
    "        best_f1_socre, best_precison, best_recall, best_predict = best_f1(score, label)\n",
    "        best_f1_socre_, best_precison_, best_recall_, best_predict_ = (\n",
    "            best_f1_without_pointadjust(score, label)\n",
    "        )\n",
    "        df[\"delay_predict\"] = delay_predict\n",
    "        df[\"best_predict\"] = best_predict\n",
    "        df.to_csv(\n",
    "            \"./csv/result.csv\",\n",
    "            index=False,\n",
    "        )\n",
    "        file_name = self.hp.save_file\n",
    "        with open(file_name, \"a\") as f:\n",
    "            f.write(\n",
    "                \"Auc %f \\nbest f1 score %f %f %f \\nDelay f1 score  %f %f %f\\nBest f1 without pointadjust %f %f %f\\n\"\n",
    "                % (\n",
    "                    auc,\n",
    "                    best_f1_socre,\n",
    "                    best_precison,\n",
    "                    best_recall,\n",
    "                    delay_f1_score,\n",
    "                    delay_precison,\n",
    "                    delay_recall,\n",
    "                    best_f1_socre_,\n",
    "                    best_precison_,\n",
    "                    best_recall_,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def mydataloader(self, mode):\n",
    "        dataset = UniDataset(\n",
    "            self.hp.use_label,\n",
    "            self.hp.window,\n",
    "            self.hp.data_dir,\n",
    "            self.hp.data_name,\n",
    "            mode,\n",
    "            self.hp.sliding_window_size,\n",
    "            data_pre_mode=self.hp.data_pre_mode,\n",
    "        )\n",
    "        train_sampler = None\n",
    "        batch_size = self.hp.batch_size\n",
    "        try:\n",
    "            if self.on_gpu:\n",
    "                train_sampler = DistributedSampler(dataset, rank=self.trainer.proc_rank)\n",
    "                batch_size = batch_size // self.trainer.world_size  # scale batch size\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        should_shuffle = train_sampler is None\n",
    "        if mode == \"valid\" or mode == \"test\":\n",
    "            should_shuffle = False\n",
    "        loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=should_shuffle,\n",
    "            sampler=train_sampler,\n",
    "            num_workers=self.hp.num_workers,\n",
    "        )\n",
    "        return loader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hp.learning_rate)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args():\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument(\n",
    "            \"--data_name\", default=\"0efb375b-b902-3661-ab23-9a0bb799f4e3.csv\", type=str\n",
    "        )\n",
    "        parser.add_argument(\"--data_dir\", default=\"./data/AIOPS/\", type=str)\n",
    "        parser.add_argument(\"--window\", default=64, type=int)\n",
    "        parser.add_argument(\"--latent_dim\", default=8, type=int)\n",
    "        parser.add_argument(\"--only_test\", default=0, type=int)\n",
    "        parser.add_argument(\"--max_epoch\", default=30, type=int)\n",
    "        parser.add_argument(\"--batch_size\", default=512, type=int)\n",
    "        parser.add_argument(\"--num_workers\", default=8, type=int)\n",
    "        parser.add_argument(\"--learning_rate\", default=0.0005, type=float)\n",
    "        parser.add_argument(\"--sliding_window_size\", default=1, type=int)\n",
    "        parser.add_argument(\"--save_file\", default=\"./result/Score.txt\", type=str)\n",
    "        parser.add_argument(\"--data_pre_mode\", default=0, type=int)\n",
    "        parser.add_argument(\"--missing_data_rate\", default=0.01, type=float)\n",
    "        parser.add_argument(\"--point_ano_rate\", default=0.05, type=float)\n",
    "        parser.add_argument(\"--seg_ano_rate\", default=0.1, type=float)\n",
    "        parser.add_argument(\"--eval_all\", default=0, type=int)\n",
    "        parser.add_argument(\"--condition_emb_dim\", default=16, type=int)\n",
    "        parser.add_argument(\"--d_model\", default=256, type=int)\n",
    "        parser.add_argument(\"--d_inner\", default=512, type=int)\n",
    "        parser.add_argument(\"--n_head\", default=8, type=int)\n",
    "        parser.add_argument(\"--kernel_size\", default=16, type=int)\n",
    "        parser.add_argument(\"--stride\", default=8, type=int)\n",
    "        parser.add_argument(\"--mcmc_rate\", default=0.2, type=float)\n",
    "        parser.add_argument(\"--mcmc_value\", default=-5, type=float)\n",
    "        parser.add_argument(\"--mcmc_mode\", default=2, type=int)  # 0 is rate 2 default\n",
    "        parser.add_argument(\n",
    "            \"--condition_mode\", default=2, type=int\n",
    "        )  # 2 both local and global\n",
    "        parser.add_argument(\"--dropout_rate\", default=0.05, type=float)\n",
    "        parser.add_argument(\"--gpu\", default=0, type=int)\n",
    "        parser.add_argument(\"--use_label\", default=0, type=int)\n",
    "        return parser\n",
    "\n",
    "    def batch_data_augmentation(self, x, y, z):\n",
    "        \"\"\"missing data injection\"\"\"\n",
    "\n",
    "        if self.hp.point_ano_rate > 0:\n",
    "            x_a, y_a, z_a = data_augment.point_ano(x, y, z, self.hp.point_ano_rate)\n",
    "            x = torch.cat((x, x_a), dim=0)\n",
    "            y = torch.cat((y, y_a), dim=0)\n",
    "            z = torch.cat((z, z_a), dim=0)\n",
    "        if self.hp.seg_ano_rate > 0:\n",
    "            x_a, y_a, z_a = data_augment.seg_ano(\n",
    "                x, y, z, self.hp.seg_ano_rate, method=\"swap\"\n",
    "            )\n",
    "            x = torch.cat((x, x_a), dim=0)\n",
    "            y = torch.cat((y, y_a), dim=0)\n",
    "            z = torch.cat((z, z_a), dim=0)\n",
    "        x, y, z = data_augment.missing_data_injection(\n",
    "            x, y, z, self.hp.missing_data_rate\n",
    "        )\n",
    "        return x, y, z\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
