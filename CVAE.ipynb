{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d08e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from Attention import EncoderLayer_selfattn\n",
    "import pywt\n",
    "\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hp,\n",
    "        gamma: float = 1000.0,\n",
    "        max_capacity: int = 25,\n",
    "        Capacity_max_iter: int = 1e5,\n",
    "        loss_type: str = \"C\",\n",
    "    ):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.hp = hp\n",
    "        self.num_iter = 0\n",
    "        self.step_max = 0\n",
    "        self.gamma = gamma\n",
    "        self.loss_type = loss_type\n",
    "        self.C_max = torch.Tensor([max_capacity])\n",
    "        self.C_stop_iter = Capacity_max_iter\n",
    "        modules = []\n",
    "        in_channels = self.hp.window + 2 * self.hp.condition_emb_dim\n",
    "        self.hidden_dims = [100, 100]\n",
    "        for h_dim in self.hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_channels, h_dim),\n",
    "                    nn.Tanh(),\n",
    "                )\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(self.hidden_dims[-1], self.hp.latent_dim)\n",
    "        self.fc_var = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dims[-1], self.hp.latent_dim),\n",
    "            nn.Softplus(),\n",
    "        )\n",
    "        modules = []\n",
    "        self.decoder_input = nn.Linear(\n",
    "            self.hp.latent_dim + 2 * self.hp.condition_emb_dim, self.hidden_dims[-1]\n",
    "        )\n",
    "        self.hidden_dims.reverse()\n",
    "        for i in range(len(self.hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(self.hidden_dims[i], self.hidden_dims[i + 1]),\n",
    "                    nn.Tanh(),\n",
    "                )\n",
    "            )\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(self.hidden_dims[-1], self.hp.window),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "        )\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "        self.fc_mu_x = nn.Linear(self.hp.window, self.hp.window)\n",
    "        self.fc_var_x = nn.Sequential(\n",
    "            nn.Linear(self.hp.window, self.hp.window), nn.Softplus()\n",
    "        )\n",
    "        self.atten = nn.ModuleList(\n",
    "            [\n",
    "                EncoderLayer_selfattn(\n",
    "                    self.hp.d_model,\n",
    "                    self.hp.d_inner,\n",
    "                    self.hp.n_head,\n",
    "                    self.hp.d_inner // self.hp.n_head,\n",
    "                    self.hp.d_inner // self.hp.n_head,\n",
    "                    dropout=0.1,\n",
    "                )\n",
    "                for _ in range(1)\n",
    "            ]\n",
    "        )\n",
    "        self.emb_local = nn.Sequential(\n",
    "            nn.Linear(2 + self.hp.kernel_size, self.hp.d_model),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.out_linear = nn.Sequential(\n",
    "            nn.Linear(self.hp.d_model, self.hp.condition_emb_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(self.hp.dropout_rate)\n",
    "        self.emb_global = nn.Sequential(\n",
    "            nn.Linear(self.hp.window, self.hp.condition_emb_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def encode(self, input):\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "        mu = self.fc_mu(result)\n",
    "        var = self.fc_var(result)\n",
    "        return [mu, var]\n",
    "\n",
    "    def decode(self, z):\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, 1, self.hidden_dims[0])\n",
    "        result = self.decoder(result)\n",
    "        mu_x = self.fc_mu_x(result)\n",
    "        var_x = self.fc_var_x(result)\n",
    "        return mu_x, var_x\n",
    "\n",
    "    def reparameterize(self, mu, var):\n",
    "        std = torch.sqrt(1e-7 + var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input, mode, y):\n",
    "        if mode == \"train\" or mode == \"valid\":\n",
    "            condition = self.get_conditon(input)\n",
    "            condition = self.dropout(condition)\n",
    "            mu, var = self.encode(torch.cat((input, condition), dim=2))\n",
    "            z = self.reparameterize(mu, var)\n",
    "            mu_x, var_x = self.decode(torch.cat((z, condition.squeeze(1)), dim=1))\n",
    "            rec_x = self.reparameterize(mu_x, var_x)\n",
    "            loss = self.loss_func(mu_x, var_x, input, mu, var, y, z)\n",
    "            return [mu_x, var_x, rec_x, mu, var, loss]\n",
    "        else:\n",
    "            y = y.unsqueeze(1)\n",
    "            return self.MCMC2(input)\n",
    "\n",
    "    def get_conditon(self, x):\n",
    "        x_g = x\n",
    "        f_global = torch.fft.rfft(x_g[:, :, :-1], dim=-1)\n",
    "        f_global = torch.cat((f_global.real, f_global.imag), dim=-1)\n",
    "        f_global = self.emb_global(f_global)\n",
    "        x_g = x_g.view(x.shape[0], 1, 1, -1)\n",
    "        x_l = x_g.clone()\n",
    "        x_l[:, :, :, -1] = 0\n",
    "        unfold = nn.Unfold(\n",
    "            kernel_size=(1, self.hp.kernel_size),\n",
    "            dilation=1,\n",
    "            padding=0,\n",
    "            stride=(1, self.hp.stride),\n",
    "        )\n",
    "        unfold_x = unfold(x_l)\n",
    "        unfold_x = unfold_x.transpose(1, 2)\n",
    "        f_local = torch.fft.rfft(unfold_x, dim=-1)\n",
    "        f_local = torch.cat((f_local.real, f_local.imag), dim=-1)\n",
    "        f_local = self.emb_local(f_local)\n",
    "        for enc_layer in self.atten:\n",
    "            f_local, enc_slf_attn = enc_layer(f_local)\n",
    "        f_local = self.out_linear(f_local)\n",
    "        f_local = f_local[:, -1, :].unsqueeze(1)\n",
    "        output = torch.cat((f_global, f_local), -1)\n",
    "        return output\n",
    "\n",
    "    def MCMC2(self, x):\n",
    "        condition = self.get_conditon(x)\n",
    "        origin_x = x.clone()\n",
    "        for i in range(10):\n",
    "            mu, var = self.encode(torch.cat((x, condition), dim=2))\n",
    "            z = self.reparameterize(mu, var)\n",
    "            mu_x, var_x = self.decode(torch.cat((z, condition.squeeze(1)), dim=1))\n",
    "            recon = -0.5 * (torch.log(var_x) + (origin_x - mu_x) ** 2 / var_x)\n",
    "            temp = (\n",
    "                torch.from_numpy(np.percentile(recon.cpu(), self.hp.mcmc_rate, axis=-1))\n",
    "                .unsqueeze(2)\n",
    "                .repeat(1, 1, self.hp.window)\n",
    "            ).to(\"cuda\")\n",
    "            if self.hp.mcmc_mode == 0:\n",
    "                l = (temp < recon).int()\n",
    "                x = mu_x * (1 - l) + origin_x * l\n",
    "            if self.hp.mcmc_mode == 1:\n",
    "                l = (self.hp.mcmc_value < recon).int()\n",
    "                x = origin_x * l + mu_x * (1 - l)\n",
    "            if self.hp.mcmc_mode == 2:\n",
    "                l = torch.ones_like(origin_x)\n",
    "                l[:, :, -1] = 0\n",
    "                x = origin_x * l + (1 - l) * mu_x\n",
    "        prob_all = 0\n",
    "        mu, var = self.encode(torch.cat((x, condition), dim=2))\n",
    "        for i in range(128):\n",
    "            z = self.reparameterize(mu, var)\n",
    "            mu_x, var_x = self.decode(torch.cat((z, condition.squeeze(1)), dim=1))\n",
    "            prob_all += -0.5 * (torch.log(var_x) + (origin_x - mu_x) ** 2 / var_x)\n",
    "        return x, prob_all / 128\n",
    "\n",
    "    def loss_func(self, mu_x, var_x, input, mu, var, y, z, mode=\"nottrain\"):\n",
    "        if mode == \"train\":\n",
    "            self.num_iter += 1\n",
    "            self.num_iter = self.num_iter % 100\n",
    "        kld_weight = 0.005\n",
    "        mu_x = mu_x.squeeze(1)\n",
    "        var_x = var_x.squeeze(1)\n",
    "        input = input.squeeze(1)\n",
    "        recon_loss = torch.mean(\n",
    "            0.5\n",
    "            * torch.mean(y * (torch.log(var_x) + (input - mu_x) ** 2 / var_x), dim=1),\n",
    "            dim=0,\n",
    "        )\n",
    "        m = (torch.sum(y, dim=1, keepdim=True) / self.hp.window).repeat(\n",
    "            1, self.hp.latent_dim\n",
    "        )\n",
    "        kld_loss = torch.mean(\n",
    "            0.5 * torch.mean(m * (z**2) - torch.log(var) - (z - mu) ** 2 / var, dim=1),\n",
    "            dim=0,\n",
    "        )\n",
    "        if self.loss_type == \"B\":\n",
    "            self.C_max = self.C_max.to(input.device)\n",
    "            C = torch.clamp(\n",
    "                self.C_max / self.C_stop_iter * self.num_iter, 0, self.C_max.data[0]\n",
    "            )\n",
    "            loss = recon_loss + self.gamma * kld_weight * (kld_loss - C).abs()\n",
    "        elif self.loss_type == \"C\":\n",
    "            loss = recon_loss + kld_loss\n",
    "        elif self.loss_type == \"D\":\n",
    "            loss = recon_loss + self.num_iter / 100 * kld_loss\n",
    "        else:\n",
    "            raise ValueError(\"Undefined loss type.\")\n",
    "        return loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
